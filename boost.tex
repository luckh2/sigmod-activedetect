\section{Repair Selection Algorithm}
The key insight of this paper is that the problem described in Section 3.3 can be addressed with statistical boosting.

\subsection{Overview of Boosting}
Ensemble methods construct predictions from combinations of predictors.
Boosting, a type of ensembling, is based on the observation that finding many ``weak learners'' is often significantly easier than finding a single, highly accurate predictor. 
The boosting algorithm calls this ``weak'' or ``base'' learning algorithm repeatedly feeding it a  weighting over the training examples.
Each time it is called, the base learning algorithm generates a new weak prediction rule, and after many rounds, the boosting algorithm must combine these weak learners into a single prediction rule that, hopefully, will be much more accurate than any one of the weak learners.

We will first introduce the classical AdaBoost algorithm for binary classifiers.
This is without a loss of generality since we can use an all-versus-one technique to handle multi-class classification.
The algorithm takes as input a training set of features and labels $(X,Y)$--assume that the labels are $\{-1, 1\}$.
AdaBoost calls a given weak learner repeatedly in a series of rounds. 
The algorithm re-weights the dataset after each round.
By training on a weighted dataset, we mean that it finds a learning from a set of permissible learners that maxmimizes the weighted accuracy.
For weighting function $W(x,y) \mapsto \mathbb{R}_+$:
\[
acc(C, W) = \frac{\sum_{x,y} W(x,y) \mathbf{1}(C((x, null)).y = y)}{\sum_{x,y} W(x,y) }
\]
Initially, all weights are set equally, but on each round, the weights of incorrectly classified examples are increased so that the learner is forced to focus on the hard examples in the training set.

Formally, the AdaBoost algorithm~\cite{freund1995desicion} proceeds as follows:
\begin{algorithm}
\KwData{(X, Y), $\alpha$}
Initialize $W^{(1)}_i = \frac{1}{N}$\\
\For{$t \in [1, T]$}{
  $C_t$ = Train weak learner on dataset weighed by $W^{t}_i$ \\
  $\epsilon_t$ = Calculate weighted classification error \\
  $\alpha_t = \ln(\frac{1-\epsilon_t}{\epsilon_t})$ \\
  $W^{(t+1)}_i \propto W^{(t)}_i e^{-\alpha_t y_i C_t(x_i)}$: down-weight correct predictions, up-weight incorrectly predictions.
}
\Return $C(x) = \text{sign}(\sum_t^T \alpha_t C_t(x) )$
\caption{AdaBoost Algorithm}
\label{alg:adaboost}
\end{algorithm}

\subsection{Why Boosting?}
The key difference from naive feature selection algorithms is that it selects over the space of models rather than the space of features.
If we have repair operations that cannot simply be represented as columnar operations (e.g., removing a record), this is a preferred solution.
Similarly, it makes few assumptions about how the attributes are aggregated into a model.

In our problem, each of the library elements will define a weak learner.
Given the dataset $R$, we can apply $l \in \mathcal{L}$ and then train the classifier to return $C_l$. 
The weak learners are evaluated on the clean test labels, which dictates weighting.
Modeling the selecting process as a statistical boosting allows us to make relatively few assumptions about the classifier and the data cleaning operations. 
Instead of having to reason about composing different data cleaning operations (and how compositions may affect accuracy), we are reasoning about a weighted consensus of classifiers trained with different data cleaning approaches.

\subsection{Repair Selection Algorithm}\label{s:boostalg}
The boosting algorithm weights the dataset depending on mispredictions, focusing future effort on the ensembles current mispredictions.
In each round, we find the $l \in \mathcal{L}$ that generates the classifier with highest test accuracy on the weighted data.
After selection, we recalculate the wights.
Repeat until $B$ cleaning operations are selected, by selecting the operation that performs best with updated weights.
The result is a new classifier $C_{clean}$ that is derived from the ensemble.
As before, without loss of generality we present the binary classification case with labels in $\{-1,1\}$.

\begin{algorithm}
\KwData{(X, Y), $\alpha$}
Initialize $W^{(1)}_i = \frac{1}{N}$\\

$\mathcal{L}$ generates a set of classifiers $\mathcal{C} \{C^{(0)}, C^{(1)},...,C^{(k)}\}$ where $C^{(0)}$ is the base classifier and $C^{(1)},...,C^{(k)}$ are derived from the cleaning operations.\\

\For{$t \in [1, T]$}{
  $C_t$ = Find $C_t \in \mathcal{C}$ that maximizes the weighted accuracy on the test set. 
  $\epsilon_t$ = Calculate weighted classification error on the test set
  $\alpha_t = \ln(\frac{1-\epsilon_t}{\epsilon_t})$ 
  $W^{(t+1)}_i \propto W^{(t)}_i e^{-\alpha_t y_i C_t(x_i)}$: down-weight correct predictions, up-weight incorrectly predictions.
}
\Return $C(x) = \text{sign}(\sum_t^T \alpha_t C_t(x) )$
\caption{Repair Selection Algorithm}
\label{alg:rsa}
\end{algorithm}

The algorithm has a few intuitive properties: (1) it prioritizes cleaning operations that improve performance, (2) if no such operations exist it does no worse than the base classifier, and (3) it is agnostic to the implementation of the classifiers.
The basic runtime of the algorithm is polynomial in both the number of cleaning operations and size of the dataset. In the next subsection, we will describe optimizations.

\begin{proposition}[Time Complexity]
The time complexity of Boost-and-Clean is $\mathbf{O}(k^2 N_{test} + k N_{train})$, where $k$ is the number of data cleaning operations, $N_{test}$ is the number of test tuples, and $N_{train}$ is the number of training tuples.
\end{proposition}

Boosting is well-understood statistically, and we can further bound the error on our clean test set (follows from~\cite{schapire2003boosting}):

\begin{proposition}[Error Bound]
For a budget of $B$ cleaning operations, the error rate of Boost-and-Clean on the test dataset decreases as $\mathbf{O}(e^{-2B})$.
\end{proposition}


