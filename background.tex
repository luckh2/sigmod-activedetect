\vspace{2em}
\section{Background}
This section motivates \sys in relation to prior work.
Using the pilot study as inspiration, consider the following running example to describe the system and notation:

\begin{example}[Lead Prediction]\sloppy\label{ex:lead}
Past clients are stored in a relational database:
\[
R(id, name, num\_emp, industry, region, successful)
\]
where $name$ is the company name, $num\_emp$ is the number of employees in the company, $industry$ is a categorical attribute that describes the industry segment, $region$ is a code indicating the region of the country the business is headquartered, and $is\_successful$ is a Boolean describing whether the company purchased the product.
\end{example}

\begin{figure}[t]
% \vspace{-5pt}
\centering
 \includegraphics[width=\columnwidth]{figures/training_and_pred_errors.png}
 \caption{The above example uses the invariant that the number of employees is greater than $0$.  Training errors are violations of the invariante in the training dataset (top row). Prediction errors are invariant violations in the test data during prediction. \sys is a tool to {\it detect} both types of errors and generate a corrective {\it repair} action.
 \label{fig:error}}
\end{figure}

\subsection{Machine Learning and Dirty Data}
It is important to emphasize that we are not concerned about statistical noise or the natural variance of data in the dataset.
Rather, we are interested in cases where an invariant that is expected to be true, i.e., a logical assertion about properties of the dataset, is violated leading unforeseen biases in the way the records are featurized.
For example, suppose we expect every $num\_emp \ne~NULL$, but the particular dataset we receive has null values.
Either the featurization program will error, or worse, it may not error and interpret $NULL$ as $0$.
However, semantically $NULL$ and $0$ refer to two very different concepts: a missing value or a company with no employees.
Conversely, one can also construct cases where two values that have the same semantic meaning are mapped to different numerical representations.
Suppose the attribute $region$ has two representations for the western United States: $USWEST$ and $USW$. 
This error would lead to two different feature values for records from the same region.

Fundamentally, performance of a model degrades when these violated invariants create a mismatch between the training and prediction environments~(Figure \ref{fig:error}):

\vspace{0.5em}\noindent\textbf{Training Errors: } Invariants that are expected to be true during prediction time are violated in the training dataset. In the running example, consider the case when $R$ is integrated from multiple previously collected datasets, where some of the datasets failed to record $num\_emp$. Assuming that there is no convenient way to retroactively determine the ground truth, the analyst has a couple of options to handle this error: (1) drop all records with a missing  $num\_emp$ value, or (2) impute $num\_emp$ with a sensible default value.

\vspace{0.25em}\noindent\textbf{Prediction Errors: } Invariants that are expected to be true during training time are violated when the model is asked to predict a label for a new example. 
Consider the case where all of the training examples recorded $num\_emp$, but a new test example has a null value.
Unlike in the training set, it may not be an acceptable solution to drop a prediction (i.e., the system fails to respond to a query).
So, the analyst has a couple different options to handle this error: (1) impute $num\_emp$ with a sensible default value and then predict, (2) apply a ``fail-safe'' prediction.

\ewu{why say ``expected to be true during training are violated in prediction'' rather than ``invariants are violated''?}

\vspace{0.5em}

Eventually, we may want to integrate the new examples into the model and re-train--turning prediction errors into future training errors.
Already, for a missing value in $num\_emp$, there are four different choices the analyst can handle a violation (2 training $\times$ 2 prediction).
These four choices have to be evaluated for each possible error type based on its severity, data type, and predictive importance.
Recent surveys of data scientists suggest that the current procedure for making these choices is manual and ad-hoc~\cite{kandel2012, krishnan2016hilda}.
As ML applications are increasingly in the critical path, management systems that are resilient to errors by automatically making such choices are important.
\ewu{Not clear what the above text is trying to argue -- that there are too many options to manually try?  that it's not clear what a ``goodness'' criteria is?  somethingelse?}

\subsection{Possible Approaches}
\sys brings together generic, dataset-independent error detection with automatically learned repair strategies for ML applications.
We review how baseline techniques proposed in prior work that could apply to this problem.

\vspace{0.5em}

\noindent\textbf{Rule-based Repair: } 
The traditional relational approach to this problem to ignore the downstream model and clean the relation completely.
In the running example, suppose some of the values for $num\_emp$ are NULL and we want to train a classifier to predict $is\_successful$.
We would define a domain integrity constraint $num\_emp \ne~NULL$, and then propose a set of repairs to satisfy this constraint.
With no other information, this rule-based approach could in principle impute any non-null value from the domain to create a logically consistent relation.
To avoid this problem, we can adopt an approach like~\cite{prokoshyna2015combining} and select the imputations that minimize the statistical distance of the updated relation to an ideal distribution for the attribute, for example, an ideal power-law distribution.
This would impute values in such a way that $num\_emp$ matched a Zipfian distribution.

On its own, this optimization approach has been empirically very successful, however, when we train a classifier on this data, counter-intuitive effects can occur.  
The data cleaning operation may break important correlations in the data and may introduce biases into the training data not present in test conditions. 
Consider the degenerate case where $num\_emp = NULL$ is perfectly correlated with one of the prediction classes--in this case, it may be better to NOT clean the data!
Similarly, consider the case where there is a very strong class imbalance.
When observing a record with $num\_emp = NULL$, rather than making a noisy prediction, it might be better to default to the most popular class. 
While more sophisticated statistical imputation techniques exist~\cite{schafer1998multiple}, they all have the same fundamental problem that the value imputation is divorced from the downstream classifier's predictive accuracy. 
We see this problem in our experiments (Section~\ref{exp:comp}), where on some datasets imputing the most frequent value leads to a more accurate downstream classifier than imputing to minimize the difference from an ideal distribution.

\vspace{0.5em}\noindent\textbf{Statistical Detection: } Rule-based techniques are dependent on the analyst defining the invariants, which might be challenging if the analyst cannot anticipate how future data might look.
There is a well-established line of literature on statistical anomaly detection~\cite{hellerstein2008quantitative}, and for the most part, these techniques are generic and dataset independent (up-to hyperparameters). Typically, such approaches identify \emph{outlier} records outside of some normal range of variance. However, the problem is that not all dirty data look like outliers. In the running example, there could truly be companies where $num\_emp = 0$. It has been shown that statistical anomaly detection techniques miss obvious errors in heterogeneous datasets with mixes of numerical, categorical, and string-valued data~\cite{DBLP:journals/pvldb/AbedjanCDFIOPST16}.

Abedjan et al. recently evaluated a wide range of error detection techniques on 5 proprietary real-world datasets~\cite{DBLP:journals/pvldb/AbedjanCDFIOPST16}.  They found that the errors in 3 of the datasets were dominated by missing cell values; 1 dataset contained functional dependency violations due to erroneous numerical attribute values, and only 1 dataset require complex user-specified denial constraints~\cite{chu2013discovering} to identify the errors.  These findings suggest that, in 4 of the 5 datasets, a significant portion of data errors can be classified as {\it domain integrity errors}, wherein a cell contains a value outside of its domain of permissible values.

We believe this highlights the potential value of automated cleaning systems such as \sys to identify the bulk of common-case errors, so that developers may focus on the specialized, domain-specific errors.  The prevalence of {\it domain integrity errors} suggests that a pre-defined set of featurizers and detector generators can be sufficient to detect these errors.  In fact, on 8 of our experimental datasets, \sys using our pre-populated detector library achieves a detection accuracy of 81\% of all of the errors found by hand-written rules.

Therefore, we need a mix of statistical rules and logic rules to determine errors.
We explore to what extent we can derive these rules from data for routine errors. 
We surveyed 8 ML datasets used in Kaggle competitions and benchmarks in the UCI ML repository, and found that a majority of the non-statistical errors could be detected as \emph{domain integrity constraints}, i.e., disallowed values in single columns.
We apply a combination of heuristic checks for missing values and data type errors, and a neural network based error detector that identifies attribute values not likely to co-occur in the same record.

\iffalse
\subsection{Solution Overview}
There does not exist one single error detector or repair action that dominates, so which ones should an analyst choose? 
\sys models this problem as a boosting problem.
Rather than thinking of each of detector and repair pair as a data transformation, it thinks of each as generating a new ``classifier'' that provides some additional information about the label.
The problem of selecting the top data pairs is equivalent to ensembling a subset of the classifiers as best as possible.
We choose a boosting framework to represent this problem due to the relatively minimal assumptions about the structure and implementation details of the user-specified classifier.
To construct the underlying library of cleaning operations we surveyed datasets on Kaggle and the UCI repository and built a library of data cleaning operations that supported common data cleaning tasks across the datasets.
\fi